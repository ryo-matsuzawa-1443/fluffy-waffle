import streamlit as st
from notion_client import Client
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import os
import zipfile
import urllib.request

# -------------------------------
# ğŸ”§ Dropboxã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼†å±•é–‹
# -------------------------------
ZIP_URL = "https://www.dropbox.com/scl/fi/izbgbhfai3w9lf9seypre/my_model.zip?rlkey=3w8l307p4xuz1c3oqbcnfxnm8&st=ookby7ig&dl=1"
ZIP_PATH = "my_model.zip"
MODEL_DIR = "./my_model"

if not os.path.exists(MODEL_DIR):
    st.info("ğŸ¤– Dropboxã‹ã‚‰AIãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ä¸­ã§ã™ã€‚å°‘ã—ãŠå¾…ã¡ãã ã•ã„...")
    urllib.request.urlretrieve(ZIP_URL, ZIP_PATH)
    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
        zip_ref.extractall(MODEL_DIR)

model = SentenceTransformer(MODEL_DIR)

# -------------------------------
# ğŸ”§ Notionãƒˆãƒ¼ã‚¯ãƒ³ã¨è¨­å®š
# -------------------------------
NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
AZS_DB_ID = "02c8dffa2f6e45c1898c36b04503bd23"
RELATION_PROP_NAME = "AZS DB"

# -------------------------------
# ğŸ”§ DB ID æŠ½å‡ºé–¢æ•°
# -------------------------------
def extract_db_id(notion_url):
    try:
        return notion_url.split("/")[-1].split("?")[0]
    except:
        return None

# -------------------------------
# ğŸ”§ å…¨ãƒšãƒ¼ã‚¸å–å¾—é–¢æ•°ï¼ˆ100ä»¶ä»¥ä¸Šå¯¾å¿œï¼‰
# -------------------------------
def get_database_items(notion, db_id):
    results = []
    next_cursor = None

    while True:
        response = notion.databases.query(
            database_id=db_id,
            start_cursor=next_cursor
        ) if next_cursor else notion.databases.query(database_id=db_id)

        results.extend(response["results"])

        if response.get("has_more"):
            next_cursor = response["next_cursor"]
        else:
            break

    return results

# -------------------------------
# ğŸ”§ ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†
# -------------------------------
def run_matching(PJ_DB_ID, threshold):
    notion = Client(auth=NOTION_TOKEN)

    azs_items = get_database_items(notion, AZS_DB_ID)
    PJ_items = get_database_items(notion, PJ_DB_ID)

    azs_names, azs_pages = [], {}
    for item in azs_items:
        name = item["properties"].get("éƒ¨å±‹å", {}).get("title", [])
        if name:
            text = name[0]["text"]["content"]
            azs_names.append(text)
            azs_pages[text] = item["id"]

    PJ_names, PJ_pages = [], {}
    for item in PJ_items:
        name = item["properties"].get("å®¤å", {}).get("title", [])
        if name:
            text = name[0]["text"]["content"]
            PJ_names.append(text)
            PJ_pages[text] = item["id"]

    azs_embeddings = model.encode(azs_names, convert_to_tensor=True)
    pj_embeddings = model.encode(PJ_names, convert_to_tensor=True)

    cosine_scores = util.pytorch_cos_sim(pj_embeddings, azs_embeddings)

    approved_matches = []
    pending_matches = []

    for i, PJ_name in enumerate(PJ_names):
        score_row = cosine_scores[i]
        best_index = int(score_row.argmax())
        best_score = float(score_row[best_index])
        best_match = azs_names[best_index]

        match_info = {
            "å®¤å": PJ_name,
            "ãƒãƒƒãƒã—ãŸéƒ¨å±‹å": best_match,
            "é¡ä¼¼åº¦": int(best_score * 100),
            "AZSãƒšãƒ¼ã‚¸ID": azs_pages[best_match],
            "æ¤œè¨¼ãƒšãƒ¼ã‚¸ID": PJ_pages[PJ_name],
        }

        if match_info["é¡ä¼¼åº¦"] >= threshold:
            approved_matches.append(match_info)
        else:
            pending_matches.append(match_info)

    for match in approved_matches:
        notion.pages.update(
            page_id=match["æ¤œè¨¼ãƒšãƒ¼ã‚¸ID"],
            properties={RELATION_PROP_NAME: {
                "relation": [{"id": match["AZSãƒšãƒ¼ã‚¸ID"]}]
            }}
        )
        st.write(f'âœ”ï¸ {match["å®¤å"]} â†’ {match["ãƒãƒƒãƒã—ãŸéƒ¨å±‹å"]}ï¼ˆã‚¹ã‚³ã‚¢: {match["é¡ä¼¼åº¦"]}ï¼‰')

    df_pending = pd.DataFrame(pending_matches)
    df_approved = pd.DataFrame(approved_matches)

    df_pending.to_csv("pending_matches.csv", index=False, encoding='utf-8-sig')
    df_approved.to_csv("approved_matches.csv", index=False, encoding='utf-8-sig')

    if pending_matches:
        st.warning("âš ï¸ é¡ä¼¼åº¦ãŒä½ãä¿ç•™ã•ã‚ŒãŸå®¤åã‚ã‚Šï¼ˆä¿ç•™ãƒãƒƒãƒãƒ³ã‚°CSV ã‚’ç¢ºèªï¼‰")
        st.dataframe(df_pending)
    else:
        st.success("ğŸ‰ ã™ã¹ã¦ã®å®¤åãŒè‡ªå‹•ãƒãƒƒãƒã•ã‚Œã¾ã—ãŸï¼")

    with open("approved_matches.csv", "rb") as f:
        st.download_button(
            label="ğŸ“¥ æ‰¿èªæ¸ˆã¿ãƒãƒƒãƒãƒ³ã‚°CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=f,
            file_name="approved_matches.csv",
            mime="text/csv"
        )

    with open("pending_matches.csv", "rb") as f:
        st.download_button(
            label="ğŸ“¥ ä¿ç•™ãƒãƒƒãƒãƒ³ã‚°CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=f,
            file_name="pending_matches.csv",
            mime="text/csv"
        )

# -------------------------------
# ğŸ–¼ï¸ Streamlit UI
# -------------------------------
st.title("ğŸ—ï¸ Notion å®¤åAIãƒãƒƒãƒãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")

url = st.text_input("ğŸ”— PJãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

threshold = st.slider(
    "ğŸ“Š é¡ä¼¼åº¦ã®ã—ãã„å€¤ï¼ˆã“ã®å€¤ä»¥ä¸Šã‚’ãƒãƒƒãƒãƒ³ã‚°å¯¾è±¡ã¨ã—ã¾ã™ï¼‰",
    min_value=0,
    max_value=100,
    value=70,
    step=1,
)

if st.button("ğŸš€ ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ"):
    db_id = extract_db_id(url)
    if db_id:
        st.info(f"âœ… DB ID å–å¾—: {db_id}")
        run_matching(db_id, threshold)
    else:
        st.error("âŒ ç„¡åŠ¹ãªURLå½¢å¼ã§ã™ã€‚Notionã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
