import streamlit as st
from notion_client import Client
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import os

NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
AZS_DB_ID = "02c8dffa2f6e45c1898c36b04503bd23"
RELATION_PROP_NAME = "AZS DB"

# -------------------------------
# âœ… ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿
# -------------------------------
MODEL_NAME = 'paraphrase-MiniLM-L6-v2'
MODEL_DIR = './.cache_model'

if not os.path.exists(MODEL_DIR):
    st.info("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã§ã™ã€‚å°‘ã—ãŠå¾…ã¡ãã ã•ã„...")
    model = SentenceTransformer(MODEL_NAME)
    model.save(MODEL_DIR)
else:
    model = SentenceTransformer(MODEL_DIR)

# -------------------------------
# ğŸ”§ é–¢æ•°ï¼šURLã‹ã‚‰DB IDã‚’æŠ½å‡º
# -------------------------------
def extract_db_id(notion_url):
    try:
        return notion_url.split("/")[-1].split("?")[0]
    except:
        return None

# -------------------------------
# ğŸ”§ é–¢æ•°ï¼š100ä»¶ä»¥ä¸Šå–å¾—å¯¾å¿œ
# -------------------------------
def get_database_items(notion, db_id):
    results = []
    next_cursor = None

    while True:
        response = notion.databases.query(
            database_id=db_id,
            start_cursor=next_cursor
        ) if next_cursor else notion.databases.query(database_id=db_id)

        results.extend(response["results"])

        if response.get("has_more"):
            next_cursor = response["next_cursor"]
        else:
            break

    return results

# -------------------------------
# ğŸ”§ é–¢æ•°ï¼šãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ
# -------------------------------
def run_matching(PJ_DB_ID, threshold):
    notion = Client(auth=NOTION_TOKEN)

    azs_items = get_database_items(notion, AZS_DB_ID)
    PJ_items = get_database_items(notion, PJ_DB_ID)

    azs_names, azs_pages = [], {}
    for item in azs_items:
        name = item["properties"].get("éƒ¨å±‹å", {}).get("title", [])
        if name:
            text = name[0]["text"]["content"]
            azs_names.append(text)
            azs_pages[text] = item["id"]

    PJ_names, PJ_pages = [], {}
    for item in PJ_items:
        name = item["properties"].get("å®¤å", {}).get("title", [])
        if name:
            text = name[0]["text"]["content"]
            PJ_names.append(text)
            PJ_pages[text] = item["id"]

    azs_embeddings = model.encode(azs_names, convert_to_tensor=True)
    pj_embeddings = model.encode(PJ_names, convert_to_tensor=True)

    cosine_scores = util.pytorch_cos_sim(pj_embeddings, azs_embeddings)

    approved_matches = []
    pending_matches = []

    for i, PJ_name in enumerate(PJ_names):
        score_row = cosine_scores[i]
        best_index = int(score_row.argmax())
        best_score = float(score_row[best_index])
        best_match = azs_names[best_index]

        match_info = {
            "å®¤å": PJ_name,
            "ãƒãƒƒãƒã—ãŸéƒ¨å±‹å": best_match,
            "é¡ä¼¼åº¦": int(best_score * 100),
            "AZSãƒšãƒ¼ã‚¸ID": azs_pages[best_match],
            "æ¤œè¨¼ãƒšãƒ¼ã‚¸ID": PJ_pages[PJ_name],
        }

        if match_info["é¡ä¼¼åº¦"] >= threshold:
            approved_matches.append(match_info)
        else:
            pending_matches.append(match_info)

    for match in approved_matches:
        notion.pages.update(
            page_id=match["æ¤œè¨¼ãƒšãƒ¼ã‚¸ID"],
            properties={RELATION_PROP_NAME: {
                "relation": [{"id": match["AZSãƒšãƒ¼ã‚¸ID"]}]
            }}
        )
        st.write(f'âœ”ï¸ {match["å®¤å"]} â†’ {match["ãƒãƒƒãƒã—ãŸéƒ¨å±‹å"]}ï¼ˆã‚¹ã‚³ã‚¢: {match["é¡ä¼¼åº¦"]}ï¼‰')

    df_pending = pd.DataFrame(pending_matches)
    df_approved = pd.DataFrame(approved_matches)

    df_pending.to_csv("pending_matches.csv", index=False, encoding='utf-8-sig')
    df_approved.to_csv("approved_matches.csv", index=False, encoding='utf-8-sig')

    if pending_matches:
        st.warning("âš ï¸ é¡ä¼¼åº¦ãŒä½ãä¿ç•™ã•ã‚ŒãŸå®¤åã‚ã‚Šï¼ˆä¿ç•™ãƒãƒƒãƒãƒ³ã‚°CSV ã‚’ç¢ºèªï¼‰")
        st.dataframe(df_pending)
    else:
        st.success("ğŸ‰ ã™ã¹ã¦ã®å®¤åãŒè‡ªå‹•ãƒãƒƒãƒã•ã‚Œã¾ã—ãŸï¼")

    with open("approved_matches.csv", "rb") as f:
        st.download_button(
            label="ğŸ“¥ æ‰¿èªæ¸ˆã¿ãƒãƒƒãƒãƒ³ã‚°CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=f,
            file_name="approved_matches.csv",
            mime="text/csv"
        )

    with open("pending_matches.csv", "rb") as f:
        st.download_button(
            label="ğŸ“¥ ä¿ç•™ãƒãƒƒãƒãƒ³ã‚°CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=f,
            file_name="pending_matches.csv",
            mime="text/csv"
        )

# -------------------------------
# ğŸ–¼ï¸ Streamlit UI
# -------------------------------
st.title("ğŸ—ï¸ Notion å®¤åAIãƒãƒƒãƒãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")

url = st.text_input("ğŸ”— PJãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

threshold = st.slider(
    "ğŸ“Š é¡ä¼¼åº¦ã®ã—ãã„å€¤ï¼ˆã“ã®å€¤ä»¥ä¸Šã‚’ãƒãƒƒãƒãƒ³ã‚°å¯¾è±¡ã¨ã—ã¾ã™ï¼‰",
    min_value=0,
    max_value=100,
    value=70,
    step=1,
)

if st.button("ğŸš€ ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ"):
    db_id = extract_db_id(url)
    if db_id:
        st.info(f"âœ… DB ID å–å¾—: {db_id}")
        run_matching(db_id, threshold)
    else:
        st.error("âŒ ç„¡åŠ¹ãªURLå½¢å¼ã§ã™ã€‚Notionã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
